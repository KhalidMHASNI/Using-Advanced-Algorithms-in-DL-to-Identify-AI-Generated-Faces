# AI vs. AI: Using Advanced Algorithms in DL to Identify AI-Generated Faces

This repository contains the code, dataset, and resources for the project **"Recognizing Faces Generated by Artificial Intelligence"**, which aims to detect AI-generated faces using deep learning techniques. The project builds on state-of-the-art methods, proposing an enhanced detection framework that improves upon existing texture-based detection models. The repository includes all necessary components for replicating the experiments and understanding the results.

## Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Dataset](#dataset)
- [Model Training](#model-training)
- [Evaluations and Results](#results)
- [Limitations](#limitations)
- [Future Work](#future-work)

## Overview

With the rise of powerful AI architectures like **StyleGAN** and **StyleGAN2**, generating photorealistic human faces has become easier. While this offers creative opportunities, it also poses significant risks when used maliciously, such as in deepfakes. This project introduces an enhanced detection framework based on texture analysis to differentiate real human faces from AI-generated ones.

The proposed framework builds upon the **PatchCraft** architecture, introducing improvements in feature extraction and classification, aiming for better performance in detecting AI-generated faces, especially those created using **StyleGAN2**.

## Project Structure

```bash
├── dataset/               # Folder containing sample real and AI-generated images
├── models/                # Pre-trained models and checkpoints
├── src/                   # Source code for model architecture, training, and evaluation
│   ├── data_preprocessing.py
│   ├── model.py
│   ├── train.py
│   ├── evaluate.py
│   └── utils.py
├── results/               # Folder containing results (confusion matrix, metrics, etc.)
├── requirements.txt       # Required packages for the project
├── README.md              # Project documentation
└── LICENSE                # License for the project
```

## Dataset

The dataset includes real and AI-generated human faces. The real images are sourced from the **FFHQ (Flickr-Faces-HQ)** dataset, while the synthetic faces are generated using **StyleGAN2**. If you want to use your own dataset, modify the `data_preprocessing.py` script accordingly.

- **FFHQ Dataset**: [Link](https://www.kaggle.com/datasets/deepakg/ffhq-face-data)
- **AI-generated faces**: Collected from [thispersondoesnotexist.com](https://thispersondoesnotexist.com).

## Model Training

To train the model on the provided dataset:

1. Configure the paths to your dataset in `train.py`.
2. Run the training script:

```bash
python src/train.py
```

Training logs, accuracy, and loss curves will be saved in the results/ folder. We used the Adam optimizer and Binary Cross-Entropy as the loss function. The training process takes about 4 days on an Nvidia RTX 4090 GPU.

## Results

The model achieved the following results:

- **Accuracy**: 82.62%
- **Precision (AI-generated)**: 89%
- **Recall (AI-generated)**: 74%
- **F1-Score (AI-generated)**: 81%

The results demonstrate strong performance in detecting AI-generated images, particularly those created with StyleGAN2. For more detailed results, including accuracy and loss curves, and visualizations such as the confusion matrix, check the `results/` folder.

## Limitations

While the model performed well, there were a few limitations:

- **Computational Cost**: Training the model on large datasets required significant computational resources, including high-end GPUs like the Nvidia RTX 4090. This makes it less accessible for researchers with limited resources.
- **Generalization**: The model's ability to generalize to other AI-generated image models (e.g., diffusion models and autoregressive models like DALL-E) was not tested. While the model was effective with StyleGAN2, future evaluations should test its performance on a wider variety of generative models.

## Future Work

Future directions for this project include:

1. **Expand Dataset**: Extend the dataset to include images generated by diffusion models (e.g., Stable Diffusion) and autoregressive models (e.g., DALL-E) to assess the model's generalization capabilities.
2. **Integrate Multi-Modal Techniques**: Explore the use of multi-modal AI detection techniques that combine different modalities (e.g., text, audio) to improve robustness and detection accuracy across various AI-generated content types.
3. **Optimize for Limited Resources**: Focus on optimizing the model's architecture and training processes to make it more efficient for use with limited computational resources, allowing for wider adoption by researchers and developers.
